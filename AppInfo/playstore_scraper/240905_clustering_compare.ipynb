{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV 파일에서 데이터 불러오기\n",
    "apps_data = pd.read_csv('cleaned_apps_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 앱 이름을 불용어 리스트로 처리\n",
    "app_names = apps_data['Name'].apply(lambda x: x.lower()).tolist()  # 앱 이름을 소문자로 변환하여 리스트로 수집\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 추가적인 불용어 정의\n",
    "additional_stopwords = ['pdf', 'file', 'app', 'lite', 'application', 'https', 'url', 'www']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 기본 불용어와 추가 불용어 결합\n",
    "stop_words = list(text.ENGLISH_STOP_WORDS.union(app_names).union(additional_stopwords))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jeleez\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 2. Sentence-BERT 모델 로드 및 문장 임베딩 생성\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Sentence-BERT 모델\n",
    "embeddings = model.encode(apps_data['Cleaned_Description'].values)  # 앱 설명에 대한 임베딩 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jeleez\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "# 2. 사용자 입력 클러스터 개수 설정\n",
    "n_clusters = 5\n",
    "\n",
    "# 4. K-Means 클러스터링 수행 (사용자가 입력한 클러스터 개수 사용)\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "clusters = kmeans.fit_predict(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 클러스터 결과를 데이터프레임에 추가\n",
    "apps_data['Cluster'] = clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordNet을 사용하여 키워드의 상위 개념을 찾는 함수\n",
    "def get_hypernym(word):\n",
    "    synsets = wn.synsets(word)\n",
    "    if synsets:\n",
    "        hypernyms = synsets[0].hypernyms()\n",
    "        if hypernyms:\n",
    "            return hypernyms[0].lemmas()[0].name()  # 상위 개념 반환\n",
    "    return word  # 상위 개념이 없으면 원래 단어 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF 기반 주요 키워드 추출 함수\n",
    "def get_top_keywords(tfidf_matrix, clusters, vectorizer, top_n=5):\n",
    "    cluster_centers = np.zeros((np.unique(clusters).size, tfidf_matrix.shape[1]))\n",
    "    \n",
    "    for cluster in np.unique(clusters):\n",
    "        cluster_centers[cluster] = tfidf_matrix[clusters == cluster].mean(axis=0)\n",
    "    \n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    top_keywords = []\n",
    "    \n",
    "    for cluster in range(cluster_centers.shape[0]):\n",
    "        center = cluster_centers[cluster]\n",
    "        top_indices = center.argsort()[::-1][:top_n]\n",
    "        keywords = [terms[i] for i in top_indices]\n",
    "        top_keywords.append(keywords)\n",
    "    \n",
    "    return top_keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. N-gram 기반 TF-IDF 설정 (bigram 사용)\n",
    "vectorizer_ngram = TfidfVectorizer(stop_words='english', max_features=1000, ngram_range=(2, 2))\n",
    "tfidf_matrix_ngram = vectorizer_ngram.fit_transform(apps_data['Description'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. LDA 기반 주제 모델링\n",
    "lda = LDA(n_components=n_clusters, random_state=42)\n",
    "lda.fit(tfidf_matrix_ngram)\n",
    "\n",
    "def get_lda_topics(lda_model, vectorizer, top_n=5):\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    topics = []\n",
    "    for idx, topic in enumerate(lda_model.components_):\n",
    "        top_indices = topic.argsort()[::-1][:top_n]\n",
    "        topic_keywords = [terms[i] for i in top_indices]\n",
    "        topics.append(topic_keywords)\n",
    "    return topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. 클러스터별 대표 단어 선택 및 WordNet으로 일반화\n",
    "def select_representative_word(keywords):\n",
    "    keyword_embeddings = model.encode(keywords)\n",
    "    similarity_matrix = cosine_similarity(keyword_embeddings)\n",
    "\n",
    "    # 각 단어의 유사도 합 계산\n",
    "    similarity_sums = similarity_matrix.sum(axis=1)\n",
    "    \n",
    "    # 유사도 합이 가장 큰 단어를 대표 단어로 선택하고, 상위 개념으로 변환\n",
    "    representative_idx = np.argmax(similarity_sums)\n",
    "    representative_word = keywords[representative_idx]\n",
    "    return get_hypernym(representative_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. 세 가지 방법으로 클러스터 대표 단어 생성\n",
    "# WordNet 기반 일반화\n",
    "top_keywords_per_cluster = get_top_keywords(tfidf_matrix_ngram, clusters, vectorizer_ngram, top_n=5)\n",
    "wordnet_based_results = [select_representative_word(keywords) for keywords in top_keywords_per_cluster]\n",
    "\n",
    "# LDA 기반 토픽 모델링\n",
    "lda_topics = get_lda_topics(lda, vectorizer_ngram, top_n=5)\n",
    "lda_based_results = [select_representative_word(topic) for topic in lda_topics]\n",
    "\n",
    "# N-gram 기반 대표 단어\n",
    "ngram_based_results = [select_representative_word(keywords) for keywords in top_keywords_per_cluster]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-gram 기반 대표 단어\n",
    "ngram_based_results = [select_representative_word(keywords) for keywords in top_keywords_per_cluster]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. 각 방법에 대한 Silhouette Score 계산\n",
    "wordnet_silhouette = silhouette_score(embeddings, clusters)\n",
    "lda_silhouette = silhouette_score(lda.transform(tfidf_matrix_ngram), clusters)\n",
    "ngram_silhouette = silhouette_score(tfidf_matrix_ngram.toarray(), clusters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordNet 기반 클러스터 대표 단어: ['https play', 'mobile phone', 'real time', 'pdf reader', 'https www']\n",
      "WordNet Silhouette Score: 0.117\n",
      "\n",
      "LDA 기반 클러스터 대표 단어: ['play google', 'free version', 'privacy policy', 'access rights', 'pdf reader']\n",
      "LDA Silhouette Score: -0.077\n",
      "\n",
      "N-gram 기반 클러스터 대표 단어: ['https play', 'mobile phone', 'real time', 'pdf reader', 'https www']\n",
      "N-gram Silhouette Score: 0.028\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 11. 결과 출력\n",
    "print(\"WordNet 기반 클러스터 대표 단어:\", wordnet_based_results)\n",
    "print(f\"WordNet Silhouette Score: {wordnet_silhouette:.3f}\\n\")\n",
    "\n",
    "print(\"LDA 기반 클러스터 대표 단어:\", lda_based_results)\n",
    "print(f\"LDA Silhouette Score: {lda_silhouette:.3f}\\n\")\n",
    "\n",
    "print(\"N-gram 기반 클러스터 대표 단어:\", ngram_based_results)\n",
    "print(f\"N-gram Silhouette Score: {ngram_silhouette:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
