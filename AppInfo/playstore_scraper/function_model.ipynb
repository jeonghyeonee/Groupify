{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26636\\3248333432.py:2: DtypeWarning: Columns (3,4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  apps_data = pd.read_csv('cleaned_apps_data.csv')\n"
     ]
    }
   ],
   "source": [
    "# CSV 파일에서 데이터 불러오기\n",
    "apps_data = pd.read_csv('cleaned_apps_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['appId', 'title', 'url', 'score', 'price', 'free', 'category',\n",
       "       'installs', 'description', 'icon'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apps_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 앱 이름을 불용어 리스트로 처리\n",
    "app_names = apps_data['title'].apply(lambda x: x.lower()).tolist()  # 앱 이름을 소문자로 변환하여 리스트로 수집\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 추가적인 불용어 정의\n",
    "additional_stopwords = ['pdf', 'file', 'app', 'lite', 'application']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 기본 불용어와 추가 불용어 결합\n",
    "stop_words = list(text.ENGLISH_STOP_WORDS.union(app_names).union(additional_stopwords))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 4. Sentence-BERT 모델 로드 및 문장 임베딩 생성\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(apps_data['description'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "# 5. K-Means 클러스터링 수행\n",
    "n_clusters = 5\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "clusters = kmeans.fit_predict(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. TF-IDF 기반 주요 키워드 추출\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "tfidf_matrix = vectorizer.fit_transform(apps_data['description'])\n",
    "\n",
    "def get_top_keywords(tfidf_matrix, clusters, top_n=5):\n",
    "    cluster_centers = np.zeros((np.unique(clusters).size, tfidf_matrix.shape[1]))\n",
    "    \n",
    "    for cluster in np.unique(clusters):\n",
    "        cluster_centers[cluster] = tfidf_matrix[clusters == cluster].mean(axis=0)\n",
    "    \n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    top_keywords = []\n",
    "    \n",
    "    for cluster in range(cluster_centers.shape[0]):\n",
    "        center = cluster_centers[cluster]\n",
    "        top_indices = center.argsort()[::-1][:top_n]\n",
    "        keywords = [terms[i] for i in top_indices]\n",
    "        top_keywords.append(keywords)\n",
    "    \n",
    "    return top_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 클러스터별 상위 5개 키워드 추출\n",
    "top_keywords_per_cluster = get_top_keywords(tfidf_matrix, clusters, top_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. 각 키워드를 임베딩하여 벡터 생성\n",
    "def get_keyword_embedding(keywords):\n",
    "    return model.encode(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Cosine Similarity를 사용하여 중심 단어 선택\n",
    "def select_representative_word(keywords):\n",
    "    keyword_embeddings = get_keyword_embedding(keywords)\n",
    "    similarity_matrix = cosine_similarity(keyword_embeddings)\n",
    "\n",
    "    # 각 단어의 유사도 합 계산\n",
    "    similarity_sums = similarity_matrix.sum(axis=1)\n",
    "    \n",
    "    # 유사도 합이 가장 큰 단어를 대표 단어로 선택\n",
    "    representative_idx = np.argmax(similarity_sums)\n",
    "    return keywords[representative_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. 클러스터별 대표 단어 생성\n",
    "def generate_category_name(keywords):\n",
    "    return select_representative_word(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 Name: phone\n",
      "Cluster 1 Name: chat\n",
      "Cluster 2 Name: reading\n",
      "Cluster 3 Name: design\n",
      "Cluster 4 Name: card\n"
     ]
    }
   ],
   "source": [
    "# 10. 클러스터별 대표 단어 생성 및 출력\n",
    "for cluster_num, keywords in enumerate(top_keywords_per_cluster):\n",
    "    category_name = generate_category_name(keywords)\n",
    "    print(f\"Cluster {cluster_num} Name: {category_name}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
